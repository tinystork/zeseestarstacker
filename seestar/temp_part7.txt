        return result

    def _process_batches(self, batch_fits_list: list[str]):
        """Reproject and accumulate each FITS file individually."""

        if not batch_fits_list or self.reference_shape is None:
            return

        mode = str(getattr(self, "stack_final_combine", "")).lower()
        default_match_bg = mode == "reproject_coadd"
        match_background = self._get_final_match_background(default=default_match_bg)
        if match_background and mode == "reproject_coadd":
            logger.info(
                "[MATCH_BG] Background matching enabled for final coadd (mode=%s)",
                getattr(self, "stack_final_combine", mode),
            )

        worker = partial(
            _reproject_worker,
            ref_wcs_header=self.ref_wcs_header,
            shape_out=self.reference_shape,
            use_gpu=self.use_gpu,
            match_background=match_background,
        )

        def _ensure_hwc(arr):
            if arr is None:
                return None
            data = np.asarray(arr, dtype=np.float32)
            if data.ndim == 2:
                data = data[..., None]
            elif (
                data.ndim == 3
                and data.shape[0] in (1, 3, 4)
                and data.shape[-1] not in (1, 3, 4)
            ):
                data = np.moveaxis(data, 0, -1)
            return data

        def _ensure_hw(arr):
            if arr is None:
                return None
            weights = np.asarray(arr, dtype=np.float32)
            if (
                weights.ndim == 3
                and weights.shape[0] in (1, 3, 4)
                and weights.shape[-1] not in (1, 3, 4)
            ):
                weights = np.moveaxis(weights, 0, -1)
            if weights.ndim == 3:
                weights = np.nanmean(weights, axis=2)
            return weights.astype(np.float32, copy=False)

        for fits_path in batch_fits_list:
            try:
                data, wht, hdr, input_wcs = self._load_and_prepare_simple(fits_path)
            except Exception:
                continue

            reproj_data, reproj_wht = worker(fits_path)
            reproj_data = _ensure_hwc(reproj_data)
            reproj_wht = _ensure_hw(reproj_wht)

            if getattr(self, "interbatch_norm_active", False):
                reproj_data, reproj_wht = self._apply_interbatch_normalization(
                    reproj_data,
                    reproj_wht,
                    context="reproject",
                )
                reproj_data = _ensure_hwc(reproj_data)
                reproj_wht = _ensure_hw(reproj_wht)
                if reproj_wht is not None:
                    mask = (reproj_wht > 0).astype(np.float32, copy=False)
                    if reproj_data.ndim == 3:
                        reproj_data *= mask[..., None]
                    else:
                        reproj_data *= mask

            if reproj_data is None:
                continue

            if reproj_wht is None:
                reproj_wht = np.ones(reproj_data.shape[:2], dtype=np.float32)

            # continuous accumulation
            self.cumulative_sum_memmap += reproj_data
            if self.cumulative_wht_memmap.shape != reproj_wht.shape:
                from numpy.lib.format import open_memmap

                self.cumulative_wht_memmap = open_memmap(
                    self.cumulative_wht_path,
                    mode="w+",
                    dtype=np.float32,
                    shape=reproj_wht.shape,
                )
                self.cumulative_wht_memmap[:] = 0.0
            self.cumulative_wht_memmap += reproj_wht
            self.cumulative_sum_memmap.flush()
            self.cumulative_wht_memmap.flush()

            if self.enable_preview:
                self._downsample_preview(reproj_data, reproj_wht)

    # --- DANS LA CLASSE SeestarQueuedStacker DANS seestar/queuep/queue_manager.py ---

    def _interbatch_start_session(self) -> None:
        """Initialise per-run inter-batch normalization state."""
        self.interbatch_norm_active = True
        self._ibn_ref_image = None
        self._ibn_ref_wht = None
        self._ibn_ref_median = None
        self._ibn_batches_seen = 0
        self._ibn_applied = 0
        self._ibn_skipped = 0
        self._ibn_last_scale = None
        self._ibn_master_ready = False
        self._ibn_candidate_pool = []
        self._ibn_candidate_limit = 8
        self._ibn_master_min = 1
        self._ibn_counter = 0
        self._ibn_bg_applied = 0
        self._ibn_feather_cache = {}
        self._ibn_min_overlap = 10000
        self._ibn_use_percentile_ratio = True
        self._ibn_percentile_value = 90.0
        self._ibn_last_failure_info = {}
        self._ibn_ref_baseline = None
        self._ibn_last_batch_baseline = None
        msg = "INFO: Inter-batch normalization (MASTER_REF + BG2D) enabled (auto-start)"
        try:
            self.update_progress(msg, "INFO")
        except Exception:
            pass
        logger.info("Inter-batch normalization (MASTER_REF + BG2D) enabled (auto-start)")

    def _interbatch_finalize_session(self) -> None:
        """Log summary and release cached reference data."""
        if not getattr(self, "interbatch_norm_active", False):
            return
        applied = getattr(self, "_ibn_applied", 0)
        skipped = getattr(self, "_ibn_skipped", 0)
        bg_applied = getattr(self, "_ibn_bg_applied", 0)
        master_ready = getattr(self, "_ibn_master_ready", False)
        seen = getattr(self, "_ibn_batches_seen", 0)
        summary = (
            f"INFO: Inter-batch normalization summary: master_ready={master_ready}, "
            f"applied_to={applied}, skipped={skipped} (BG2D applied={bg_applied}), "
            f"batches_seen={seen}"
        )
        try:
            self.update_progress(summary, "INFO")
        except Exception:
            pass
        logger.info(
            "Inter-batch normalization summary: master_ready=%s applied_to=%d skipped=%d bg2d=%d batches_seen=%d",
            master_ready,
            applied,
            skipped,
            bg_applied,
            seen,
        )
        self.interbatch_norm_active = False
        self._ibn_ref_image = None
        self._ibn_ref_wht = None
        self._ibn_ref_median = None
        self._ibn_batches_seen = 0
        self._ibn_applied = 0
        self._ibn_skipped = 0
        self._ibn_last_scale = None
        self._ibn_master_ready = False
        self._ibn_candidate_pool = []
        self._ibn_feather_cache = {}
        self._ibn_last_failure_info = {}
        self._final_combine_ibn_started = False
        self._final_combine_ibn_master_set = False
        self._final_combine_ibn_master_batch_idx = None

    def _apply_interbatch_normalization(
        self,
        batch_data,
        batch_wht,
        *,
        context: str = "classic",
        batch_num: int | None = None,
    ):
        """Apply background removal, photometric normalization, and feathering."""
        if not getattr(self, "interbatch_norm_active", False):
            return batch_data, batch_wht
        if batch_data is None:
            return batch_data, batch_wht

        data = batch_data
        weights = batch_wht

        if not isinstance(data, np.ndarray):
            data = np.asarray(data, dtype=np.float32)
        elif data.dtype != np.float32:
            data = data.astype(np.float32)

        if weights is not None:
